# <img src="https://github.com/fodof91/fodof91/blob/main/data-analysis.png" width="40" height="40"/> Отчет о ДЗ по математической статистике 
### Вариант 2 Калабай Михаил, Порфирьеа Антон, Фирсов Федор


<img align="left" width="120" src="https://github.com/devicons/devicon/blob/master/icons/python/python-original.svg" />

#### Инструменты 
 Для выполнения расчетов мы использовали язык Python с библиотеками pandas, numpy, sklearn, statsmodels и т.д. и среды Google Colab. 
 Эти инструменты специально созданы и активно используются для работы с большими данными, методами математической статистики и машинного обучения

## <img src="https://github.com/fodof91/fodof91/blob/main/data.png" width="25" height="25"/> Задание 6
## <img src="https://github.com/fodof91/fodof91/blob/main/data.png" width="25" height="25"/> Задание 7
## <img src="https://github.com/fodof91/fodof91/blob/main/data.png" width="25" height="25"/> Задание 8
Для каждой из 3-х полученных выше моделей предзакажем вероятность совершения преступления для всех строк, в том числе и последних 50. 
Чтобы удобно использовать эту информацию в будущем сохраним ее в новой таблице -  [result_data.csv](https://github.com/fodof91/matstat/blob/main/result_data.csv)

В этой таблице появилось 3 новых столбца: res_linear, res_logit, res_probit - вероятность совершения преступления от соответствующей модели.

<img src="https://github.com/fodof91/matstat/blob/main/8.1.png"/>

Для получения 15 людей с максимальной вероятностью повторного совершения преступления, исключаем из таблицы преступников, которые уже рецидивисты и находим 15 максимумов для каждого нового столбца.
Получаем следующих преступников:
|модель|1|2|3|4|5|6|7|8|9|10|11|12|13|14|15|
|------|-|-|-|-|-|-|-|-|-|--|--|--|--|--|--|
Линейная регрессия | 385 | 927 | 653 | 409 | 588 | 201 | 667 | 408 | 800 | 187 | 522 | 824 | 230 | 874 | 931
Логит-модель | 385 | 927 | 653 | 409 | 588 | 408 | 667 | 201 | 800 | 187 | 824 | 522 | 874 | 230 | 931
Пробит-модель | 385 | 927 | 653 | 409 | 588 | 201 | 667 | 408 | 800 | 187 | 522 | 824 | 230 | 874 | 931

Как видно в таблице почти во всех прогнозах модели сошлись.

Для сравнения моделей мы  нашли попарные максимальные и средние различия(модуля) предсказаний
<table>
<tr><th>Максимальное различие </th><th>Среднее различие </th></tr>
<tr><td>

|        |Линейная |Логит    |Пробит   |
|--------|---------|---------|---------|
|Линейная|         |0.1857368|0.1749230|
|Логит   |0.1857368|         |0.0139028|
|Пробит  |0.1749230|0.0139028|         |

</td><td>

|        |Линейная |Логит    |Пробит   |
|--------|---------|---------|---------|
|Линейная|         |0.0050922|0.0038368|
|Логит   |0.0050922|         |0.0015908|
|Пробит  |0.0038368|0.0015908|         |
  
</td></tr> </table>

Как видно в таблице максимальное различие 2-x процентов, среднее различие вообще ничтожно, что гооврит о почти идентичных предсказаниях моделей.

## <img src="https://github.com/fodof91/fodof91/blob/main/data.png" width="25" height="25"/> Задание 9
Для теста отношения правдоподобия нам понадобится длинная модели - probit-модель из пунктов ранее и  короткая модель (с ограничениями).
Для этого создадим модель содержащую только константу, которая никак не зависит от остальных столбцов таблицы, кроме результата.

Проверять гипотезу будем на уровне значимости 5%

**Статистика отношения правдоподобия** $LR = -2 \cdot (ln(L_0) - ln(L_1))$
Получаем 

$LR = 49.13920$ и $P-value = 2.13084 * 10^{-8}$

Как видим p-value < LR, f а значит можно отвергнуть гипотезу о том, что модель в целом не значима. 

___Значит probit модель значима.___

## <img src="https://github.com/fodof91/fodof91/blob/main/data.png" width="25" height="25"/> Задание 10

Для исследования пороговых значений, logit-модели были использованы следующие термины:

<img src="https://media.springernature.com/lw685/springer-static/image/art%3A10.1186%2Fs13174-018-0087-2/MediaObjects/13174_2018_87_Fig4_HTML.png" width="500"/> 

В данных терминах, предложенные в задачах метрики буду иметь следуюший вид:

**Чувствительность** = $\frac{TP}{TP + FN}$

**Специфичность** = $\frac{TN}{TN + FP}$

Сделав перебор пороговых значений между 0 и 1 с шагом 0.01, были получены следующие результаты:
![download](https://github.com/fodof91/matstat/assets/90344389/773b49f3-5221-423b-b060-1e7ba0311062)

Эти графики выглядят достаточно правдоподобно, ведь из определений очевидно, что если рассмотреть наши метрики как функции от порогов, то они будут обладать следующими свойствами:

+ Чувствительность
  - Монотонно убывающая (не строго)
  - В 0 имеет значение 1
  - В 1 имеет значение 0
 
+ Специфичность
  - Монотонно возрастающая (не строго)
  - в 0 имеет значение 0
  - в 1 имеет значение 1

Эти свойства подтверждается графиками выше.

Также выделив те пороги, где чувствительность не ниже 80%, был выделен порог с максимальным значением специфичности.

Этот порог имеет значение 0.33. Чувствительность $\approx$ 80.21%. Специфичность $\approx$ 35.60%
